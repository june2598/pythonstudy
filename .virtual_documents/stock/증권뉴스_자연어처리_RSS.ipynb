import pandas as pd
import numpy as np


# 시각화 패키지
import matplotlib.pyplot as plt
import seaborn as sns
import koreanize_matplotlib
from wordcloud import WordCloud


# 모든 컬럼,행을 출력하도록 설정
pd.set_option('display.max_columns', None) # None으로 설정하면 모든 컬럼 출력 default:20
pd.set_option('display.max_rows', 10) #None으로 설정하면 모든 행 출력 default: 50

# 모든 열의 최대 너비를 설정
pd.set_option('display.max_colwidth', None) #None으로 설정하면 모든 내용을 출력 default:60


# 정규화
import re


# 날짜는 파이썬 표준 라이브러리 datetime 사용
import datetime
# 오늘 날짜 가져오기
today = datetime.datetime.today()
print(today)
today = today.strftime('%Y-%m-%d')
today


df = pd.read_csv(f'news_{today}.csv')
df.head(1)


df.shape


df.info()


df.describe()


# 중복된 행이 있는지 확인
df.duplicated()


df.duplicated().sum()


df['작성일시'].value_counts().sort_values(ascending=False).head().index


df[df.duplicated(subset='작성일시')].sort_values(by='작성일시')


df.duplicated().describe()


df['언론사'].value_counts()





# 제목,내용 길이
df['제목_길이'] = df['제목'].str.len()
df['내용_길이'] = df['내용'].str.len()


# 제목,내용 토큰 갯수
# split() : 공백을 구분자로 list로 반환해주기때문에 str 두번써야됨
df['제목_토큰_갯수'] = df['제목'].str.split().str.len()
df['내용_토큰_갯수'] = df['내용'].str.split().str.len()


df[['제목_길이','제목_토큰_갯수','내용_길이','내용_토큰_갯수']].describe()


df.info()


df.hist(bins=10)


df[df['제목_길이'] == df['제목_길이'].max()]


df[df['제목_길이'] == df['제목_길이'].max()]['제목'].values





# 제목을 하나의 문자열로 만들기 (제목과 제목사이에 공백을 하나 줌)
news_title = ' '.join(df['제목'])

# 문자열도 인덱싱/슬라이싱이 가능
news_title[:100]


WordCloud?


def display_word_cloud(str, max_words=30, width=1200,height=600) : 

    # r스트링 쓰는이유 : \ 를 특별한 의미로 해석하지말고 평범한 하나의 문자로 해석하게 하기위함
    font_path = r'C:\Windows\Fonts\malgun.ttf'
    stopwords = ['코스피','코스닥','종목','ETF','주식','주가','상승','하락','상장','투자','서학','동학','기업','시장'
                 '시총','목표가','올해','내년','국내','해외','외국인','소식에','기대감','증시','한국']
    word_cloud = WordCloud(font_path = font_path,
              width=width,
              height=height,
              stopwords=stopwords,
              background_color='white',
              min_word_length = 2,
              max_words = max_words,
              random_state=2024).generate(str)
    plt.imshow(word_cloud)
    plt.show()
    return word_cloud


display_word_cloud(news_title,50)


# tmp_str = ''.join(df[df['제목'].str.contains('제주항공')]['내용'])
# tmp_word_cloud = display_word_cloud(tmp_str,50)


tmp_str = ''.join(df[df['제목'].str.contains('이마트')]['내용'])
tmp_word_cloud = display_word_cloud(tmp_str,100)


tmp_word_cloud.words_


tmp_str = ''.join(df[df['제목'].str.contains('트럼프')]['내용'])
tmp_word_cloud = display_word_cloud(tmp_str,50)


tmp_str = ''.join(df[df['제목'].str.contains('환율')]['내용'])
tmp_word_cloud = display_word_cloud(tmp_str,50)


tmp_word_cloud.words_





# !pip install pecab


from pecab import PeCab
pecab = PeCab()


dir(pecab)


text = '아버지가 방에 들어가신다 주말 여행 영남권'


print(pecab.morphs(text))
print(pecab.nouns(text))
print(pecab.pos(text))
# print(pecab.postprocessor(text))


# 처리시간이 걸리는 작업의 프로세스 진행률을 보여줌

from tqdm import tqdm
tqdm.pandas()


df['제목_명사'] = df['제목'].progress_map(lambda x : ' '.join(pecab.nouns(x)))


# pecab은 데이터가 많을경우는 속도가 너무 느림
# df['내용_명사'] = df['내용'].progress_map(lambda x : ' '.join(pecab.nouns(x)))








from konlpy.tag import Okt


okt = Okt()


dir(okt)


txt = '아버지 방에 들어가신다'
okt.morphs(txt)


okt.pos(txt)


okt.nouns(txt)


df['내용_명사'] = df['내용'].progress_map(lambda x : ' '.join(okt.nouns(x)))


str = ''.join(df['내용_명사'])

dwc = display_word_cloud(str,100)


tmp = df.loc[df[df['내용_명사'].str.contains('테슬라')].index].head(1)['내용'].values[0]
type(tmp)


display_word_cloud(tmp,30)


len(okt.pos(tmp))


unique_words_dict = {}
for word, tag in okt.pos(tmp):
    if word not in unique_words_dict:
        unique_words_dict[word] = tag

# 딕셔너리의 길이 (고유한 단어 수)
len(unique_words_dict)


# 중복된 어휘 제거
unique_words = []
pos_list = []
for word, tag in okt.pos(tmp) :
    if word not in unique_words : 
        unique_words.append(word)
        pos_list.append(tag)

len(unique_words)
for item in zip(unique_words,pos_list) :
    print(item[0], item[1])


# for word, tag in pecab.pos(tmp) :
    # if word == '통해' :
    #     print(word, tag)
    


df.columns


df.to_csv(f'news_preprocessed_{today}.csv',index=False, encoding='utf-8-sig') 처음부터 여기까지 수동으로 shift+enter 에러는 무시할것


dwc.words_


okt.tagset?


from sklearn.feature_extraction.text import CountVectorizer


cvect = CountVectorizer()


X = cvect.fit_transform(df['제목_명사'])


tmp = pd.DataFrame(X.toarray(), columns=cvect.get_feature_names_out())


tmp.sum().sort_values(ascending=False)
